{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import necessary libraries\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note filepaths\n",
    "PATH_TRAIN_LABELS = '../datasets/train_labels.csv'\n",
    "PATH_TRAIN_RAW = '../datasets/train_images.npy'\n",
    "PATH_TEST_RAW = '../datasets/test_images.npy'\n",
    "PATH_TRAIN = '../datasets/train_images_cropped.npy'\n",
    "PATH_TEST = '../datasets/test_images_cropped.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import datasets \n",
    "train_labels = np.array(pd.read_csv(PATH_TRAIN_LABELS, delimiter=\",\", header=0, index_col=0))\n",
    "train_images = np.load(PATH_TRAIN, encoding=\"latin1\")\n",
    "test_images = np.load(PATH_TEST, encoding=\"latin1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note constants\n",
    "IMG_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocessing' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-91e835359322>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m         'pillow','parrot','squiggle','mouth','empty','pencil'])\n\u001b[1;32m      6\u001b[0m \u001b[0mcat1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcat0\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcat0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mlb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLabelBinarizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mlb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcat0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'preprocessing' is not defined"
     ]
    }
   ],
   "source": [
    "# define labels\n",
    "cat0 = sorted(['sink','pear','moustache','nose','skateboard','penguin','peanut','skull','panda',\n",
    "        'paintbrush','nail','apple','rifle','mug','sailboat','pineapple','spoon','rabbit',\n",
    "        'shovel','rollerskates','screwdriver','scorpion','rhinoceros','pool','octagon',\n",
    "        'pillow','parrot','squiggle','mouth','empty','pencil'])\n",
    "cat1 = {i: cat0[i] for i in range(len(cat0))}\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "lb.fit(cat0)\n",
    "\n",
    "# check labels\n",
    "print(cat0)\n",
    "print()\n",
    "print(cat1)\n",
    "print()\n",
    "print(lb.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_bounding_image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-e36a0573952f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mnew_x_train_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfind_largest_count_ratio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-e36a0573952f>\u001b[0m in \u001b[0;36mnew_data\u001b[0;34m(data, method)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# get subimgs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0msub_images\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mget_bounding_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;31m#find the largest one\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mlargest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_bounding_image' is not defined"
     ]
    }
   ],
   "source": [
    "def new_data(data,method):\n",
    "    ''' \n",
    "    function to go from dataset --> upto 3 preprocessed subimages \n",
    "    return the X given a method of choosing between the subimages\n",
    "    (for example using find_largest_count or looking at ratios)\n",
    "    we go from raw data to data that just has one subimage included\n",
    "    input: raw image data\n",
    "    return: chosen subimages data\n",
    "    '''\n",
    "    new_data = []\n",
    "    for image in data:\n",
    "        # get subimgs\n",
    "        sub_images =  get_bounding_image(image.reshape(64,64))\n",
    "        #find the largest one\n",
    "        largest = method(sub_images)    \n",
    "        #resize it\n",
    "        dst = rescale(largest)\n",
    "        #dst = resizeAndPad(largest, (28,28), padColor= 0) # previous way of rescaling img\n",
    "        #save it\n",
    "        new_data.append(dst)\n",
    "    return np.asarray(new_data)\n",
    "\n",
    "def find_largest_count_ratio(train_images):\n",
    "    '''\n",
    "    returns largest subimage based on the ratio of pixel counts between the actual subimage and the MNIST scaled version\n",
    "    '''\n",
    "    count_ratios=[]\n",
    "    for img in images:\n",
    "        pre_scaled_count = count_pixels_gray(img)\n",
    "        post_scaled = rescale(img)\n",
    "        post_scaled_count = count_pixels_gray(post_scaled)\n",
    "        ratio = pre_scaled_count/post_scaled_count\n",
    "        count_ratios.append( ratio )\n",
    "    # find the max image based on ratio\n",
    "    max_val = max(count_ratios)\n",
    "    index = count_ratios.index(max_val) \n",
    "    return images[index]\n",
    "\n",
    "new_x_train_data = new_data(train_images, find_largest_count_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'new_x_train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-0666a1bd8e94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;31m# Generate some data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_x_train_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0mT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_y_wo_touching\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'new_x_train_data' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Simplistic implementation of the two-layer neural network.\n",
    "Training method is stochastic (online) gradient descent with momentum.\n",
    "As an example it computes XOR for given input.\n",
    "Some details:\n",
    "- tanh activation for hidden layer\n",
    "- sigmoid activation for output layer\n",
    "- cross-entropy loss\n",
    "Less than 100 lines of active code.\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "\n",
    "n_hidden = 28\n",
    "n_in = 784\n",
    "n_out = 10\n",
    "n_samples = 300\n",
    "\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "def flatten(data):\n",
    "    flattened_data = []\n",
    "    for entry in data:\n",
    "    flattened_data.append(entry.flatten())\n",
    "    return np.asarray(flattened_data)\n",
    "\n",
    "\n",
    "def Sigmoid_forward(X):\n",
    "     return 1.0 / (1.0 + np.exp(-X))\n",
    "\n",
    "def Softmax_loss(X, y):\n",
    "    num_examples = X.shape[0]\n",
    "    probs = Softmax_predict(X)\n",
    "    data_loss = 0\n",
    "    for i in range(num_examples):\n",
    "        corect_logprobs = -np.log(probs[i]*y[i])\n",
    "        data_loss += corect_logprobs\n",
    "    return 1./num_examples * data_loss\n",
    "    #loss = -np.mean ( true * np.log(Y) + (1 - true) * np.log(1 - Y) )\n",
    "    #return loss\n",
    "\n",
    "def Softmax_predict(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "def Softmax_diff(X, y):\n",
    "    num_examples = X.shape[0]\n",
    "    probs = Softmax_predict(X)\n",
    "    probs[range(num_examples), y] -= 1\n",
    "    return probs\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1.0 + np.exp(-x))\n",
    "\n",
    "def tanh_prime(x):\n",
    "    return  1 - np.tanh(x)**2\n",
    "\n",
    "def softmax_loss_naive(W, X, y, reg= 0):\n",
    "    \"\"\"\n",
    "    Softmax loss function, naive implementation (with loops)\n",
    "    Inputs:\n",
    "    - W: C x D array of weights \n",
    "    - X: D x N array of data. Data are D-dimensional columns\n",
    "    - y: 1-dimensional array of length N with labels 0...K-1, for K classes for us the one hot encoded\n",
    "    - reg: (float) regularization strength\n",
    "    Returns:\n",
    "    a tuple of:\n",
    "    - loss as single float\n",
    "    - gradient with respect to weights W, an array of same size as W\n",
    "    \"\"\"\n",
    "    # Initialize the loss and gradient to zero.\n",
    "    loss = 0.0\n",
    "    dW = np.zeros_like(W)\n",
    "\n",
    "    #############################################################################\n",
    "    # Compute the softmax loss and its gradient using explicit loops.           #\n",
    "    # Store the loss in loss and the gradient in dW. If you are not careful     #\n",
    "    # here, it is easy to run into numeric instability. Don't forget the        #\n",
    "    # regularization!                                                           #\n",
    "    #############################################################################\n",
    "\n",
    "    # Get shapes\n",
    "    num_classes = W.shape[0]\n",
    "    num_train = X.shape[1]\n",
    "\n",
    "    for i in range(num_train):\n",
    "        # Compute vector of scores\n",
    "        f_i = W.dot(X[:, i]) # in R^{num_classes}\n",
    "\n",
    "        # Normalization trick to avoid numerical instability, per http://cs231n.github.io/linear-classify/#softmax\n",
    "        log_c = np.max(f_i)\n",
    "        f_i -= log_c\n",
    "\n",
    "        # Compute loss (and add to it, divided later)\n",
    "        # L_i = - f(x_i)_{y_i} + log \\sum_j e^{f(x_i)_j}\n",
    "        sum_i = 0.0\n",
    "        for f_i_j in f_i:\n",
    "          sum_i += np.exp(f_i_j)\n",
    "        loss += -f_i[y[i]] + np.log(sum_i)\n",
    "\n",
    "        # Compute gradient\n",
    "        # dw_j = 1/num_train * \\sum_i[x_i * (p(y_i = j)-Ind{y_i = j} )]\n",
    "        # Here we are computing the contribution to the inner sum for a given i.\n",
    "        for j in range(num_classes):\n",
    "            p = np.exp(f_i[j])/sum_i\n",
    "            dW[j, :] += (p-(j == y[i])) * X[:, i]\n",
    "\n",
    "    # Compute average\n",
    "    loss /= num_train\n",
    "    dW /= num_train\n",
    "\n",
    "    # Regularization\n",
    "    loss += 0.5 * reg * np.sum(W * W)\n",
    "    dW += reg*W\n",
    "\n",
    "    return loss, dW\n",
    "\n",
    "\n",
    "def train(x, true, V, W, bv, bw):\n",
    "\n",
    "    # forward\n",
    "    print (x.shape)\n",
    "    print (np.dot(x, V).shape)\n",
    "\n",
    "    A = np.dot(x, V) + bv\n",
    "    Z = np.tanh(A)\n",
    "\n",
    "    B = np.dot(Z, W) + bw\n",
    "    Y = Softmax_predict(B)\n",
    "    print (A.shape)\n",
    "    print (Y.shape)\n",
    "\n",
    "    # backward\n",
    "    Ew, dW = softmax_loss_naive(W,x, true)\n",
    "    print (Ew.shape)\n",
    "    Ev = tanh_prime(A) * np.dot(W, Ew)\n",
    "\n",
    "    #dW = Softmax_dW(x,true)\n",
    "    dV = np.outer(x, Ev)\n",
    "\n",
    "    loss = -np.mean ( true * np.log(Y) + (1 - true) * np.log(1 - Y) )\n",
    "\n",
    "    # Note that we use error for each layer as a gradient\n",
    "    # for biases\n",
    "\n",
    "    return  loss, (dV, dW, Ev, Ew)\n",
    "\n",
    "def predict(x, V, W, bv, bw):\n",
    "    A = np.dot(x, V) + bv\n",
    "    B = np.dot(np.tanh(A), W) + bw\n",
    "    return (sigmoid(B) > 0.5).astype(int)\n",
    "\n",
    "\n",
    "# Setup initial parameters\n",
    "V = np.random.normal(scale=0.1, size=(n_in, n_hidden))\n",
    "W = np.random.normal(scale=0.1, size=(n_hidden, n_out))\n",
    "bv = np.zeros(n_hidden)\n",
    "bw = np.zeros(n_out)\n",
    "params = [V,W,bv,bw]\n",
    "\n",
    "# Generate some data\n",
    "X = flatten(new_x_train_data)\n",
    "T = keras.utils.to_categorical(train_images, 10)\n",
    "\n",
    "# Train\n",
    "for epoch in range(100):\n",
    "    err = []\n",
    "    upd = [0]*len(params)\n",
    "    t0 = time.clock()\n",
    "    loss, grad = train(X, T, *params)\n",
    "    params -= upd\n",
    "    upd = learning_rate * grad + upd\n",
    "    err.append( loss )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
