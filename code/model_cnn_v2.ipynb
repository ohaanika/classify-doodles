{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ohaanika/classify-doodles/blob/master/model_cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XJtGwzKHxtIG"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tflearn\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d, avg_pool_2d\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.estimator import regression\n",
    "import tensorflow as tf\n",
    "\n",
    "TRAIN_DIR = '../datasets/train_images.npy'\n",
    "TRAIN_LABEL_DIR = '../datasets/train_labels.csv'\n",
    "TEST_DIR = '../datasets/test_images.npy'\n",
    "TRAIN_DIR_PRE = '../datasets/train_images_cropped.npy'\n",
    "TEST_DIR_PRE = '../datasets/test_images_cropped.npy'\n",
    "IMG_SIZE = 100\n",
    "LR = 1e-4\n",
    "\n",
    "MODEL_NAME  = 'hand-drawn-{}-{}.model'.format(LR, '2conv-basic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "q9dki06wjT4S",
    "outputId": "23bcb4e0-00e8-4df9-f3a8-5a8ba6427f6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['sink', 0], ['pear', 1], ['moustache', 2], ['nose', 3], ['skateboard', 4], ['penguin', 5], ['peanut', 6], ['skull', 7], ['panda', 8], ['paintbrush', 9], ['nail', 10], ['apple', 11], ['rifle', 12], ['mug', 13], ['sailboat', 14], ['pineapple', 15], ['spoon', 16], ['rabbit', 17], ['shovel', 18], ['rollerskates', 19], ['screwdriver', 20], ['scorpion', 21], ['rhinoceros', 22], ['pool', 23], ['octagon', 24], ['pillow', 25], ['parrot', 26], ['squiggle', 27], ['mouth', 28], ['empty', 29], ['pencil', 30]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cate = ['sink','pear','moustache','nose','skateboard','penguin'\n",
    "              ,'peanut','skull','panda','paintbrush','nail','apple',\n",
    "              'rifle','mug','sailboat','pineapple','spoon','rabbit',\n",
    "              'shovel','rollerskates','screwdriver','scorpion','rhinoceros'\n",
    "              ,'pool','octagon','pillow','parrot','squiggle','mouth',\n",
    "               'empty','pencil']\n",
    "categories = {0:'apple',1:'empty',2:'moustache', 3:'mouth', 4:'mug', 5:'nail', 6:'nose'\\\n",
    "              ,7:'octagon', 8:'paintbrush', 9:'panda', 10:'parrot',11:'peanut',\\\n",
    "              12:'pear', 13:'pencil', 14:'penguin',15:'pillow',16:'pineapple',17:'pool'\\\n",
    "              ,18:'rabbit', 19:'rhinoceros',20:'rifle',21:'rollerskates',22:'sailboat',\\\n",
    "              23:'scorpion',24:'screwdriver', 25:'shovel', 26:'sink', 27:'skateboard',\\\n",
    "              28:'skull',29:'spoon',30:'squiggle'}\n",
    "from sklearn import preprocessing\n",
    "#This will give a 1-of-k coding scheme with alphbat sequence\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "lb.fit(cate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dt9xGTuVxtIO"
   },
   "outputs": [],
   "source": [
    "#Notice here preprocessed image as input\n",
    "train_row = np.load(TRAIN_DIR_PRE, encoding = 'latin1')\n",
    "train_label = np.array(pd.read_csv(TRAIN_LABEL_DIR, delimiter=\",\"))\n",
    "final_test = np.load(TEST_DIR_PRE,encoding='latin1')\n",
    "#train = train_data[:7000]\n",
    "#valid = train_data[7000:9000]\n",
    "#test = train_data[9000:]\n",
    "train = train_data[:9000]\n",
    "valid = train_data[9000:9950]\n",
    "test = train_data[9950:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "id": "O5p9yl0Khwg4",
    "outputId": "b0e60550-0456-4250-9e26-34d2e8245dd9"
   },
   "outputs": [],
   "source": [
    "train_labels = train_label[:,1]\n",
    "train_x = train_row[:,1]\n",
    "train_data = []\n",
    "for i in range(len(train_labels)):\n",
    "    encoded = lb.transform([train_labels[i]])\n",
    "    train_data.append([np.array(train_x[i]), encoded[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8ipOCjIUxtIR",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "convent = input_data(shape = [None, IMG_SIZE, IMG_SIZE, 1],name = 'input')\n",
    "\n",
    "convent = conv_2d(convent, nb_filter = 40, filter_size = [5,5], activation = 'relu')\n",
    "convent = max_pool_2d(convent, 2)\n",
    "\n",
    "convent = conv_2d(convent, nb_filter = 64, filter_size = [2,2], activation = 'relu')\n",
    "convent = max_pool_2d(convent, 2)\n",
    "\n",
    "convent = conv_2d(convent, nb_filter = 20, filter_size = [2,2], activation = 'relu')\n",
    "convent = max_pool_2d(convent, 2)\n",
    "\n",
    "#convent = fully_connected(convent, 1024, activation = 'relu')\n",
    "convent = dropout(convent, 0.8)\n",
    "\n",
    "convent = fully_connected(convent, 31, activation ='softmax')\n",
    "convent = regression(convent, optimizer = 'Adam', learning_rate = LR, loss='categorical_crossentropy', name = 'target')\n",
    "\n",
    "model = tflearn.DNN(convent, tensorboard_dir = 'log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EA-tJGn_xtIW"
   },
   "outputs": [],
   "source": [
    "if os.path.exists('{}.meta'.format(MODEL_NAME)):\n",
    "    model.load(MODEL_NAME)\n",
    "    print('model loaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PM5PSMGTZFIw"
   },
   "outputs": [],
   "source": [
    "X = np.array([i[0] for i in train]).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "Y = [i[1] for i in train]\n",
    "Y = np.reshape(Y,(-1,31))\n",
    "\n",
    "\n",
    "valid_x = np.array([i[0] for i in valid]).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "valid_y = np.array([i[1] for i in valid]).reshape(-1,31)\n",
    "\n",
    "test_x = np.array([i[0] for i in test]).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "test_y = np.array([i[1] for i in test]).reshape(-1,31)\n",
    "\n",
    "# the shape of preprocessed test set is (10000,2) idx 0: img, idx 1:id\n",
    "final_test = final_test[:,1]\n",
    "final_test_x = np.array([i for i in final_test]).reshape(-1,IMG_SIZE,IMG_SIZE,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "9AxzMX1iZo94",
    "outputId": "6157bc65-e280-44ff-b6bc-334749a1c403",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1649  | total loss: \u001b[1m\u001b[32m0.88002\u001b[0m\u001b[0m | time: 31.743s\n",
      "| Adam | epoch: 015 | loss: 0.88002 - acc: 0.7278 -- iter: 6976/7000\n",
      "Training Step: 1650  | total loss: \u001b[1m\u001b[32m0.89207\u001b[0m\u001b[0m | time: 36.411s\n",
      "| Adam | epoch: 015 | loss: 0.89207 - acc: 0.7206 | val_loss: 8.67389 - val_acc: 0.0665 -- iter: 7000/7000\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "model.fit(X, Y, n_epoch = 15, validation_set = (valid_x,valid_y),\n",
    "          snapshot_step=200, show_metric=True, run_id=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "d1v2boVQcFNB",
    "outputId": "6c482f3c-dd30-42ac-b772-86c45a5b5b6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:/home/ubuntu/Documents/comp551/classify-doodles/hand-drawn-0.001-6conv-basic.model is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    }
   ],
   "source": [
    "model.save(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qMA7GzO03LvV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.584\n"
     ]
    }
   ],
   "source": [
    "acc = 0\n",
    "for num, data in enumerate(test):\n",
    "    img_num = data[1]\n",
    "    img_data = data[0]\n",
    "  \n",
    "    orig = img_data.reshape(IMG_SIZE, IMG_SIZE)\n",
    "    data = img_data.reshape(IMG_SIZE, IMG_SIZE, 1)\n",
    "  \n",
    "    model_out = cnn_model.predict([data])[0]  \n",
    "    prediction = model_out.argmax(axis=-1)\n",
    "  \n",
    "    true = img_num.argmax(axis=-1)\n",
    "    if(prediction == true): acc+=1\n",
    "result = acc/(len(test))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = cnn_model.predict(final_test_x[:3000]).argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_p2 = cnn_model.predict(final_test_x[3000:6000]).argmax(axis=-1)\n",
    "pred_test_p3 = cnn_model.predict(final_test_x[6000:]).argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = np.hstack([pred_test,pred_test_p2,pred_test_p3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open(\"../output/submit_03.csv\",\"w\") as f:\n",
    "    wt = csv.writer(f,delimiter=',')\n",
    "    wt.writerow(['Id','Category'])\n",
    "    for idx,i in enumerate(pred_test):\n",
    "        wt.writerow([str(idx),str(categories.get(i))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten\n",
    "from keras.optimizers import *\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import pprint\n",
    "def cnn_tuning(rate_num, l_rate):\n",
    "    tf.reset_default_graph()\n",
    "    convent = input_data(shape = [None, IMG_SIZE, IMG_SIZE, 1],name = 'input')\n",
    "    convent = conv_2d(incoming = convent, nb_filter = 32, filter_size = [5,5], padding ='valid', activation ='relu')\n",
    "    convent = max_pool_2d(incoming = convent, kernel_size = [2,2], strides =[2,2])\n",
    "    convent = conv_2d(incoming = convent, nb_filter = 64, filter_size = [5,5], padding ='valid', activation ='relu')\n",
    "    convent = max_pool_2d(incoming = convent, kernel_size = [2,2], strides =[2,2])\n",
    "    convent = conv_2d(incoming = convent, nb_filter = 32, filter_size = [5,5], padding ='valid', activation ='relu')\n",
    "    convent = max_pool_2d(incoming = convent, kernel_size = [2,2], strides =[2,2])\n",
    "    convent = conv_2d(incoming = convent, nb_filter = 64, filter_size = [5,5], padding ='valid', activation ='relu')\n",
    "    convent = max_pool_2d(incoming = convent, kernel_size = [2,2], strides =[2,2])\n",
    "    #convent = conv_2d(incoming = convent, nb_filter = 64, filter_size = [3,3], padding ='valid', activation ='relu')\n",
    "    #convent = avg_pool_2d(incoming = convent, kernel_size = [2,2], strides =[2,2])\n",
    "    #convent = conv_2d(incoming = convent, nb_filter = 64, filter_size = [3,3], padding ='valid', activation ='relu')\n",
    "    #convent = avg_pool_2d(incoming = convent, kernel_size = [2,2], strides =[2,2])\n",
    "    \n",
    "    dropout = tf.layers.dropout(inputs = convent, rate =rate_num)\n",
    "    convent = fully_connected(convent, 31, activation = 'softmax')\n",
    "    convent = regression(convent, optimizer = 'adam', learning_rate = l_rate, loss = 'categorical_crossentropy', name = 'target')\n",
    "    model = tflearn.DNN(convent, tensorboard_dir = 'log')\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2114  | total loss: \u001b[1m\u001b[32m0.63017\u001b[0m\u001b[0m | time: 20.852s\n",
      "| Adam | epoch: 015 | loss: 0.63017 - acc: 0.8433 -- iter: 8960/9000\n",
      "Training Step: 2115  | total loss: \u001b[1m\u001b[32m0.63925\u001b[0m\u001b[0m | time: 22.037s\n",
      "| Adam | epoch: 015 | loss: 0.63925 - acc: 0.8371 | val_loss: 2.24005 - val_acc: 0.5768 -- iter: 9000/9000\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "cnn_model = cnn_tuning(0.3,1e-4)\n",
    "cnn_model.fit(X, Y, n_epoch = 15, validation_set = (valid_x,valid_y),\n",
    "              snapshot_step=200, show_metric=True, run_id=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb1 = [32, 64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 879  | total loss: \u001b[1m\u001b[32m0.23962\u001b[0m\u001b[0m | time: 10.545s\n",
      "| Adam | epoch: 008 | loss: 0.23962 - acc: 0.9592 -- iter: 6976/7000\n",
      "Training Step: 880  | total loss: \u001b[1m\u001b[32m0.26303\u001b[0m\u001b[0m | time: 11.668s\n",
      "| Adam | epoch: 008 | loss: 0.26303 - acc: 0.9601 | val_loss: 3.52627 - val_acc: 0.5340 -- iter: 7000/7000\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "dropout_rate = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "with open('result_dropout.csv','w') as f:\n",
    "    f.write(\"dropout,accuracy\")\n",
    "    f.write('\\n')\n",
    "    for element in dropout_rate:\n",
    "        cnn_model = cnn_tuning(element,1e-4)\n",
    "        cnn_model.fit(X, Y, n_epoch = 8, validation_set = (valid_x,valid_y),\n",
    "                  snapshot_step=200, show_metric=True, run_id=MODEL_NAME)\n",
    "        result = cnn_model.evaluate(test_x, test_y)\n",
    "        f.write(str(element))\n",
    "        f.write(',')\n",
    "        f.write(str(result))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 651  | total loss: \u001b[1m\u001b[32m0.38218\u001b[0m\u001b[0m | time: 9.786s\n",
      "\u001b[2K\r",
      "| Adam | epoch: 006 | loss: 0.38218 - acc: 0.9155 -- iter: 6464/7000\n"
     ]
    }
   ],
   "source": [
    "epoch = [8, 12, 15, 20, 25]\n",
    "with open('result_epoch.csv','w') as f:\n",
    "    f.write(\"epoch,accuracy\")\n",
    "    f.write('\\n')\n",
    "    for element in epoch:\n",
    "        cnn_model = cnn_tuning(0.3, 1e-4)\n",
    "        cnn_model.fit(X, Y, n_epoch = element, validation_set = (valid_x,valid_y),\n",
    "                  snapshot_step=200, show_metric=True, run_id=MODEL_NAME)\n",
    "        result = cnn_model.evaluate(test_x, test_y)\n",
    "        f.write(str(element))\n",
    "        f.write(',')\n",
    "        f.write(str(result))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2749  | total loss: \u001b[1m\u001b[32m0.03061\u001b[0m\u001b[0m | time: 10.395s\n",
      "| Adam | epoch: 025 | loss: 0.03061 - acc: 0.9962 -- iter: 6976/7000\n",
      "Training Step: 2750  | total loss: \u001b[1m\u001b[32m0.03175\u001b[0m\u001b[0m | time: 11.512s\n",
      "| Adam | epoch: 025 | loss: 0.03175 - acc: 0.9950 | val_loss: 5.20567 - val_acc: 0.5425 -- iter: 7000/7000\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "snapshot_step = [50, 100, 150, 200, 300]\n",
    "with open('result_snapshot_step.csv','w') as f:\n",
    "    f.write(\"snapshot_step,accuracy\")\n",
    "    f.write('\\n')\n",
    "    for element in snapshot_step:\n",
    "        cnn_model = cnn_tuning(0.3, 1e-4)\n",
    "        cnn_model.fit(X, Y, n_epoch = 25, validation_set = (valid_x,valid_y),\n",
    "                  snapshot_step=element, show_metric=True, run_id=MODEL_NAME)\n",
    "        result = cnn_model.evaluate(test_x, test_y)\n",
    "        f.write(str(element))\n",
    "        f.write(',')\n",
    "        f.write(str(result))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2749  | total loss: \u001b[1m\u001b[32m2.53048\u001b[0m\u001b[0m | time: 9.509s\n",
      "| Adam | epoch: 025 | loss: 2.53048 - acc: 0.6897 -- iter: 6976/7000\n",
      "Training Step: 2750  | total loss: \u001b[1m\u001b[32m2.54260\u001b[0m\u001b[0m | time: 10.634s\n",
      "| Adam | epoch: 025 | loss: 2.54260 - acc: 0.6958 | val_loss: 5.34064 - val_acc: 0.4130 -- iter: 7000/7000\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "learning_rate = [1e-3, 1e-4, 1e-5]\n",
    "with open('result_learning_rate.csv','w') as f:\n",
    "    f.write(\"learning_rate,accuracy\")\n",
    "    f.write('\\n')\n",
    "    for element in learning_rate:\n",
    "        cnn_model = cnn_tuning(0.3, element)\n",
    "        cnn_model.fit(X, Y, n_epoch = 25, validation_set = (valid_x,valid_y),\n",
    "                  snapshot_step=200, show_metric=True, run_id=MODEL_NAME)\n",
    "        result = cnn_model.evaluate(test_x, test_y)\n",
    "        f.write(str(element))\n",
    "        f.write(',')\n",
    "        f.write(str(result))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = [20, 50, 100, 200]\n",
    "epochs = [5, 10, 15, 20]\n",
    "initilizer = ['lecun_uniform', 'normal', 'he_normal', 'he_uniform']\n",
    "learning_rate = [0.01, 0.1, 0.3]\n",
    "dropout_rate = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "num_unit = [5,10]\n",
    "activation = ['relu', 'tanh', 'sigmoid', 'hard_sigmoid','linear']\n",
    "optimizer = ['SGD', 'Adam', 'Adamax', 'Adagrad']\n",
    "\n",
    "# parameters = dict(batch_size = batch_size, epochs = epochs, \n",
    "#                  dropout_rate = dropout_rate, num_unit = num_unit,\n",
    "#                  initilizer = initilizer, learning_rate = learning_rate,\n",
    "#                  activation = activation, optimizer = optimizer)\n",
    "parameters = dict(dropout_rate= dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "#new_model = KerasClassifier(build_fn = build_model, verbose =0)\n",
    "models = GridSearchCV(estimator = cnn_model, param_grid = parameters, scoring = 'accuracy')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "“model_cnn.ipynb”",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
